{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: network in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# install network for the needed libray\n",
    "! pip install network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# to install nltk\n",
    "! pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk # for text function\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import network as nx # for network graphs\n",
    "\n",
    "from nltk import ngrams # continue sequence of n items\n",
    "from nltk.corpus import stopwords  # the,is,and, are the example ofstopwords \n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"stopwords\",quiet=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21288\\936221289.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df = pd.read_csv('Iliad.txt', sep='\\t').dropna().drop(\"gutenberg_id\",1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE ILIAD OF HOMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RENDERED INTO ENGLISH BLANK VERSE.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EDWARD EARL OF DERBY.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PREFACE.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text\n",
       "0                   THE ILIAD OF HOMER\n",
       "2   RENDERED INTO ENGLISH BLANK VERSE.\n",
       "3                                   BY\n",
       "5                EDWARD EARL OF DERBY.\n",
       "10                            PREFACE."
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Iliad.txt', sep='\\t').dropna().drop(\"gutenberg_id\",1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rendered into english blank verse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>edward earl of derby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>preface</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text\n",
       "0                  the iliad of homer\n",
       "2   rendered into english blank verse\n",
       "3                                  by\n",
       "5                edward earl of derby\n",
       "10                            preface"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text =text.replace(\"'\",\"\")\n",
    "    text= re.sub(r'[^\\w]',' ',text)  # leave only word charcters\n",
    "    text = re.sub(r'\\s+',' ',text)   # omit extra soace charcters\n",
    "    text=text.strip()\n",
    "    return text\n",
    "\n",
    "df['text']=df['text'].map(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>wordpairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "      <td>(t, h, e)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "      <td>(h, e,  )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "      <td>(e,  , i)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "      <td>( , i, l)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "      <td>(i, l, i)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "      <td>(l, i, a)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "      <td>(i, a, d)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "      <td>(a, d,  )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "      <td>(d,  , o)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the iliad of homer</td>\n",
       "      <td>( , o, f)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text  wordpairs\n",
       "0  the iliad of homer  (t, h, e)\n",
       "0  the iliad of homer  (h, e,  )\n",
       "0  the iliad of homer  (e,  , i)\n",
       "0  the iliad of homer  ( , i, l)\n",
       "0  the iliad of homer  (i, l, i)\n",
       "0  the iliad of homer  (l, i, a)\n",
       "0  the iliad of homer  (i, a, d)\n",
       "0  the iliad of homer  (a, d,  )\n",
       "0  the iliad of homer  (d,  , o)\n",
       "0  the iliad of homer  ( , o, f)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['wordpairs'] = df['text'].map(lambda x: list(ngrams(x,3)))\n",
    "df = df.explode('wordpairs')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text         727748\n",
      "wordpairs    727737\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "( , t, h)    15557\n",
       "(t, h, e)    14534\n",
       "(h, e,  )    12093\n",
       "(n, d,  )     7117\n",
       "(a, n, d)     6630\n",
       "             ...  \n",
       "( , t, r)     1201\n",
       "(t, h, o)     1197\n",
       "(e,  , o)     1196\n",
       "(r, s,  )     1194\n",
       "(e, s, t)     1192\n",
       "Name: wordpairs, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the most frequently used three letters word\n",
    "df['wordpairs'].value_counts().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>3rdword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t</td>\n",
       "      <td>h</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h</td>\n",
       "      <td>e</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i</td>\n",
       "      <td>l</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>l</td>\n",
       "      <td>i</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d</td>\n",
       "      <td></td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>o</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word1 word2 3rdword\n",
       "0     t     h       e\n",
       "1     h     e        \n",
       "2     e             i\n",
       "3           i       l\n",
       "4     i     l       i\n",
       "5     l     i       a\n",
       "6     i     a       d\n",
       "7     a     d        \n",
       "8     d             o\n",
       "9           o       f"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to convert into individual columns\n",
    "df = pd.DataFrame(df.wordpairs.values.tolist(),columns=['word1','word2','3rdword']).dropna()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>3rdword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h</td>\n",
       "      <td>e</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>f</td>\n",
       "      <td></td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>r</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>e</td>\n",
       "      <td>r</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word1 word2 3rdword\n",
       "1      h     e        \n",
       "11     f             h\n",
       "16     r     e       n\n",
       "20     e     r       e\n",
       "29           e       n"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove the stopwords\n",
    "en_stopwords= set(stopwords.words('english'))\n",
    "df= df[~(df.word1.isin(en_stopwords) | df.word2.isin(en_stopwords) | df['3rdword'].isin(en_stopwords))]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134882, 3)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "import tokenize\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "text = \"Natural Language Processing (NLP) is a field of computer science and artificial intelligence that deals with the interaction between computers and humans using natural language. Python is a popular programming language for NLP due to its powerful libraries and tools such as NLTK, spaCy, and TextBlob.\"\n",
    "\n",
    "stop_words= set(stopwords.words('english'))\n",
    "#words = word_tokenize(text)\n",
    "#sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hadn', 'are', 'him', 'do', 'we', 'about', 'needn', \"you'll\", 'wouldn', 'than', 'while', 'me', \"you're\", 'don', 'did', 've', \"won't\", 'so', 'a', 'myself', 'ma', 'her', 'having', 'to', 'ourselves', 'its', 'there', 'who', 'she', 'being', 'your', 'the', 'through', 'hers', \"needn't\", 'can', 'but', 'you', 'up', \"mustn't\", 'into', 'how', 'once', 'for', 'itself', 'them', 'be', 'weren', 'that', 'few', 'most', 'aren', 'if', 'when', 'd', \"wasn't\", 'an', 'didn', \"haven't\", \"wouldn't\", 'it', 'what', 'at', 'nor', \"shouldn't\", 'out', 'haven', 'only', 'this', 'have', 'same', 'has', 'll', 'here', 'should', 'our', 'was', 'they', 'o', 'their', 'mightn', 'now', 'until', \"weren't\", 'as', 're', 'those', 'against', 'all', 'such', \"isn't\", \"aren't\", 'from', 'off', 'my', 'shan', 'which', 'on', 'below', \"hadn't\", 'further', 'won', 'by', 'over', 'were', 'y', 'where', 'before', 'herself', \"should've\", 'am', 'had', 'will', 'does', 'why', 'in', 'm', 'wasn', 'yourself', \"doesn't\", 'between', \"you'd\", 'himself', \"she's\", \"that'll\", 'is', 'no', \"shan't\", 'shouldn', 'hasn', 'too', 'themselves', 'theirs', 'because', 'couldn', 'yourselves', 'down', 'then', 'other', 'doing', 'very', 'his', 'more', 'these', 'during', 'ain', \"don't\", 'isn', 'own', \"didn't\", 'doesn', 'ours', 'he', 'not', \"hasn't\", 's', 'under', \"couldn't\", 'mustn', 'again', \"it's\", 'of', 'both', 'each', \"you've\", 'after', 'whom', \"mightn't\", 'any', 'or', 'just', 'i', 'yours', 'above', 't', 'with', 'and', 'been', 'some'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', 'and', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'Python', 'is', 'a', 'popular', 'programming', 'language', 'for', 'NLP', 'due', 'to', 'its', 'powerful', 'libraries', 'and', 'tools', 'such', 'as', 'NLTK', ',', 'spaCy', ',', 'and', 'TextBlob', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing (NLP) is a field of computer science and artificial intelligence that deals with the interaction between computers and humans using natural language.', 'Python is a popular programming language for NLP due to its powerful libraries and tools such as NLTK, spaCy, and TextBlob.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words=[word for word in words if word.casefold not in stop_words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_words=[ps.stem(word) for word in filtered_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " '(',\n",
       " 'nlp',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'comput',\n",
       " 'scienc',\n",
       " 'and',\n",
       " 'artifici',\n",
       " 'intellig',\n",
       " 'that',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interact',\n",
       " 'between',\n",
       " 'comput',\n",
       " 'and',\n",
       " 'human',\n",
       " 'use',\n",
       " 'natur',\n",
       " 'languag',\n",
       " '.',\n",
       " 'python',\n",
       " 'is',\n",
       " 'a',\n",
       " 'popular',\n",
       " 'program',\n",
       " 'languag',\n",
       " 'for',\n",
       " 'nlp',\n",
       " 'due',\n",
       " 'to',\n",
       " 'it',\n",
       " 'power',\n",
       " 'librari',\n",
       " 'and',\n",
       " 'tool',\n",
       " 'such',\n",
       " 'as',\n",
       " 'nltk',\n",
       " ',',\n",
       " 'spaci',\n",
       " ',',\n",
       " 'and',\n",
       " 'textblob',\n",
       " '.']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.74, 'pos': 0.26, 'compound': 0.9136}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment = sia.polarity_scores(text)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[239], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m G \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mfrom_pandas_edgelist(df[df\u001b[39m.\u001b[39;49mn \u001b[39m>\u001b[39m\u001b[39m25\u001b[39m],\u001b[39m'\u001b[39m\u001b[39mword1\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mword2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[0;32m      3\u001b[0m nx\u001b[39m.\u001b[39mdraw_shell(G,with_labels\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,node_color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m,font_size\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "G = nx.from_pandas_edgelist(df[df.n >25],'word1','word2')\n",
    "plt.figure(figsize=(12,10))\n",
    "nx.draw_shell(G,with_labels=True,node_color='red',font_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-storage-blob\n",
      "  Downloading azure_storage_blob-12.15.0-py3-none-any.whl (387 kB)\n",
      "     -------------------------------------- 387.8/387.8 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting azure-core<2.0.0,>=1.26.0\n",
      "  Downloading azure_core-1.26.3-py3-none-any.whl (174 kB)\n",
      "     ------------------------------------ 174.5/174.5 kB 617.4 kB/s eta 0:00:00\n",
      "Collecting cryptography>=2.1.4\n",
      "  Downloading cryptography-39.0.2-cp36-abi3-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 6.8 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.0.1\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting isodate>=0.6.1\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 41.7/41.7 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting requests>=2.18.4\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from azure-core<2.0.0,>=1.26.0->azure-storage-blob) (1.16.0)\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-1.15.1-cp311-cp311-win_amd64.whl (179 kB)\n",
      "     ------------------------------------- 179.0/179.0 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "     -------------------------------------- 118.7/118.7 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp311-cp311-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.7/96.7 kB ? eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.5/61.5 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.9/140.9 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 kB 9.7 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, typing-extensions, pycparser, isodate, idna, charset-normalizer, certifi, requests, cffi, cryptography, azure-core, azure-storage-blob\n",
      "Successfully installed azure-core-1.26.3 azure-storage-blob-12.15.0 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-3.1.0 cryptography-39.0.2 idna-3.4 isodate-0.6.1 pycparser-2.21 requests-2.28.2 typing-extensions-4.5.0 urllib3-1.26.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# installing azure storage blob\n",
    "pip install azure-storage-blob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Close/Last    Volume      Open      High        Low\n",
      "0  03/16/2023    $155.85  76254420   $152.16   $156.46    $151.64\n",
      "1  03/15/2023    $152.99  77167870   $151.19  $153.245    $149.92\n",
      "2  03/14/2023    $152.59  73695890   $151.28   $153.40    $150.10\n",
      "3  03/13/2023    $150.47  84457120  $147.805   $153.14    $147.70\n",
      "4  03/10/2023    $148.50  68572400   $150.21   $150.94  $147.6096\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient,BlobClient\n",
    "import io\n",
    "\n",
    "connection_string=\"DefaultEndpointsProtocol=https;AccountName=aneritrainingc339;AccountKey=TW0zW3Y/8FbgkLlUhrlKxu7Obr4jOD7M69y94VtMoIRUTAdKWxmztYcjn7NuU8HJytEDkMy4JoEd+AStkFBaSQ==;EndpointSuffix=core.windows.net\"\n",
    "container_name=\"c339\"\n",
    "\n",
    "blob_service_client= BlobServiceClient.from_connection_string(connection_string)\n",
    "blob_client = blob_service_client.get_blob_client(container_name, \"apple.csv\")\n",
    "\n",
    "blob_data =blob_client.download_blob()\n",
    "data = blob_data.content_as_text()\n",
    "df=pd.read_csv(io.StringIO(data))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Close/Last    Volume      Open      High        Low\n",
      "0  03/16/2023    $155.85  76254420   $152.16   $156.46    $151.64\n",
      "1  03/15/2023    $152.99  77167870   $151.19  $153.245    $149.92\n",
      "2  03/14/2023    $152.59  73695890   $151.28   $153.40    $150.10\n",
      "3  03/13/2023    $150.47  84457120  $147.805   $153.14    $147.70\n",
      "4  03/10/2023    $148.50  68572400   $150.21   $150.94  $147.6096\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient,BlobClient\n",
    "import io\n",
    "\n",
    "connection_string=\"DefaultEndpointsProtocol=https;AccountName=aneritrainingc339;AccountKey=TW0zW3Y/8FbgkLlUhrlKxu7Obr4jOD7M69y94VtMoIRUTAdKWxmztYcjn7NuU8HJytEDkMy4JoEd+AStkFBaSQ==;EndpointSuffix=core.windows.net\"\n",
    "container_name=\"c339\"\n",
    "\n",
    "blob_service_client= BlobServiceClient.from_connection_string(connection_string)\n",
    "blob_client = blob_service_client.get_blob_client(container_name, \"March20/apple_data.csv\")\n",
    "\n",
    "blob_data =blob_client.download_blob()\n",
    "data = blob_data.content_as_text()\n",
    "df=pd.read_csv(io.StringIO(data))\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
